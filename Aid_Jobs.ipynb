{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "60f36a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from datetime import datetime\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import torch\n",
    "import selenium\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3637b31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome()\n",
    "browser.get(\"https://www.developmentaid.org/jobs/search?hiddenAdvancedFilters=1\")\n",
    "content = browser.page_source\n",
    "soup = BeautifulSoup(content, \"html.parser\")\n",
    "content = soup.find_all()\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3981dd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_tag = soup.find('script', {'type': 'application/ld+json'})\n",
    "# Extract and parse the JSON data\n",
    "if script_tag:\n",
    "    job_data = json.loads(script_tag.string)\n",
    "\n",
    "    # Extract the requirements\n",
    "    requirements = job_data.get(\"description\", \"\")\n",
    "    \n",
    "    # Create another BeautifulSoup object to parse HTML within the JSON\n",
    "    soup_requirements = BeautifulSoup(requirements, 'html.parser')\n",
    "    plain_text = soup_requirements.get_text(strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "66578f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "836199d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(description):\n",
    "    outputs = []\n",
    "    questions = ['What languages are required?','What education is required?',\n",
    "                'How much experience is required?','Where does this project take place?',\n",
    "                'What are the duties of this role?','What experiences are desirable?']\n",
    "    for q in questions: \n",
    "        QA_input = {'question': q,\n",
    "                'context': description}\n",
    "        outputs.append(nlp(QA_input)['answer'])\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4c878795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_net(name=\"\",job_types=\"\",languages=\"\",region=\"\",minimum_experience=\"\",maximum_experience=\"\",recency=\"\",sectors=\"\",types=\"\"):\n",
    "    url = \"https://www.developmentaid.org/jobs/search?sort=highlighted.desc,postedDate.desc\"\n",
    "    if name != '':\n",
    "        name = name.replace(' ','%20')\n",
    "        name = \"&organizationName=\"+name\n",
    "    if job_types != '':\n",
    "        job_types=job_types.replace(' ','%20')\n",
    "        job_types = \"&jobTypes=\"+job_types\n",
    "    if languages != '':\n",
    "        language_list = languages.split(\",\")\n",
    "        language_codes = []\n",
    "        for lang in language_list:\n",
    "            if lang in language_dict.keys():\n",
    "                lang = str(language_dict[lang])\n",
    "                language_codes.append(lang)\n",
    "            else:\n",
    "                next\n",
    "        addition = \"\"\n",
    "        languages = ''\n",
    "        if language_codes != []:\n",
    "            for code in language_codes:\n",
    "                addition = addition + code + ','\n",
    "            languages = \"&languages=\"+addition[0:-1]\n",
    "    if region != '':\n",
    "        if region in region_dict.keys():\n",
    "            region = str(region_dict[region])\n",
    "            region=region.replace(' ','%20')\n",
    "            region = \"&locations=\"+region\n",
    "        else:\n",
    "            region = \"\"\n",
    "    if minimum_experience != '':\n",
    "        minimum_experience=minimum_experience.replace(' ','%20')\n",
    "        minimum_experience = \"&minimumExperience=\"+minimum_experience\n",
    "    if maximum_experience != '':\n",
    "        maximum_experience=maximum_experience.replace(' ','%20')\n",
    "        maximum_experience = \"&maximumExperience=\"+maximum_experience\n",
    "    if recency != '':\n",
    "        recency=recency.replace(' ','%20')\n",
    "        recency = \"&postedDateLessThanDaysAgo=\"+recency\n",
    "    if sectors != '':\n",
    "        sectors=sectors.replace(' ','%20')\n",
    "        sectors = \"&sectors=\"+sectors\n",
    "    if types != '':\n",
    "        types=types.replace(' ','%20')\n",
    "        types = \"&types=\"+types\n",
    "    return url+name+job_types+languages+region+minimum_experience+maximum_experience+recency+sectors+types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "990f2cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_name = ['Abaza','Abkhaz','Acehnese','Adyghe','Afar','Afrikaans','Akan','Aklan','Albanian','Algonquin','Altay','Amharic','Angika','Arabic','Aramaic','Armenian','Assamese','Ateso','Awadhi','Aymara','Azerbaijani','Balinese','Balochi','Balti','Bambara','Bangla','Banjar','Banyumasan','Bashkir','Basque','Batak Toba','Beja','Belarusian','Bengali','Bhili','Bhojpuri','Bikol','Bosnian','Brahui','Buginese','Bukusu','Bulgarian','Burmese','Buryat','Catalan','Cebuano','Chaouia','Chavacano','Chechen','Chhattisgarhi','Chichewa','Chinese','Chittagonian','Chuvash','Comorian','Creole','Croatian','Czech','Danish','Dargin','Dari','Daur','Dhivehi','Dida','Dioula','Dogri','Dutch','Dzongkha','Eastern Yugur','Egyptian','English','Erzya','Estonian','Even','Ewe','Fijian','Filipino','Finnish','Fon','French','Fula','Fur','Ga','Gayo','Georgian','German','Gilaki','Gilbertese','Gondi','Greek','Guarani','Gujarati','Gusii','Hausa','Hebrew','Hiligaynon','Hindi','Hmong','Ho','Hungarian','Iban','Ibanag','Ibibio','Icelandic','Igbo','Ikalanga','Ilokano','Indonesian','Ingush','Irish','Isan','Italian','Japanese','Jarai','Javanese','Kabardian','Kabyle','Kalmyk','Kankanaey','Kannada','Kapampangan','Karachay-Balkar','Karakalpak','Kashmiri','Kazakh','Khandeshi','Khasi','Khmer','Khowar','Kinaray-a','Kinyarwanda','Kirombo','Kirundi','Kivunjo','Komi','Kongo','Konkani','Korean','Korku','Koya','Kumaoni','Kumyk','Kurdish','Kurukh','Kwanyama','Kyrgyz','Lao','Latin','Latvian','Lezgian','Lingala','Lithuanian','Lori','Lozi','Luganda','Lunda','Lusoga','Macedonian','Magahi','Maguindanao','Maithili','Makassar','Makhuwa','Makhuwa-Meetto','Malagasy','Malay','Malayalam','Maltese','Malvi','Mam','Mandinka','Mapudungun','Maranao','Marathi','Mari','Masaba','Masbateno','Mazandarani','Meitei','Minangkabau','Moksha','Mon','Mongolian','Montenegrin','Nahuatl','Nama','Ndonga','Nepal Bhasa','Nepali','Norwegian','Nuer','Oriya','Oromo','Ossetic','Pangasinan','Papiamento','Pashto','Persian/Farsi','Polish','Portuguese','Pothohari','Punjabi','Qashqai','Quechua','Rajasthani','Romani','Romanian','Russian','Sakha','Samoan','Sango','Sanskrit','Santali','Saraiki','Sardinian','Saurashtra','Serbian','Serbo-Croatian','Shina','Shona','Sicilian','Sidamo','Sign (Language)','Silesian','Silt\\'e','Sindhi','Sinhalese','Slovak','Slovenian','Soddo','Somali','Sora','Sotho','Spanish','Sundanese','Supyire','Surigaonon','Surinamese','Susu','Swahili','Swati','Swedish','Syriac','Tagalog','Tahitian','Tajik','Talysh','Tamil','Tarifit','Tashelhiyt','Tatar','Tausug','Telugu','Tetum','Thai','Tibetan','Tigre','Tigrinya','Tiv','Tok Pisin','Tonga','Tongan','Tshiluba','Tsonga','Tswana','Tuareg','Tulu','Tumbuka','Turkish','Turkmen','Tuvaluan','Tuvan','Udmurt','Ukrainian','Urdu','Uyghur','Uzbek','Venda','Vietnamese','Visayan','Welsh','Wolof','Xhosa','Yiddish','Yoruba','Yucatec Maya','Zapotec','Zazaki','Zoque','Zulu']\n",
    "language_code = [23,24,25,26,27,28,29,30,31,32,33,35,36,37,38,39,40,41,43,44,45,46,47,48,49,57,50,51,52,53,54,55,56,2,58,59,60,61,62,63,64,65,66,67,314,68,70,69,71,72,74,73,75,76,77,115,78,79,5,81,311,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,109,108,110,111,112,113,114,116,117,118,119,120,121,122,124,125,123,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,159,158,160,161,162,163,164,166,165,167,168,169,312,170,171,172,173,177,174,175,176,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,200,199,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,82,219,220,221,222,223,224,225,227,226,228,301,229,230,313,231,246,232,233,236,237,235,239,240,241,34,242,243,244,245,247,248,249,250,251,238,253,255,256,257,254,258,259,260,261,262,264,265,266,267,268,269,263,270,271,272,273,274,275,276,277,278,309,279,280,281,282,283,284,285,286,287,288,289,290,291,292,20,294,295,296,297,42,21,299,300,302,308,303,304,305,306,307]\n",
    "\n",
    "language_dict = {}\n",
    "for i in range(0,len(language_name)):\n",
    "    language_dict[language_name[i]]=language_code[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2c70d103",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_dict = {\"Africa\":3,\n",
    "    \"Asia\":5,\n",
    "    \"Europe\":6,\n",
    "    \"Latin America and the Caribbean\":4,\n",
    "    \"Northern America\":9,\n",
    "    \"Oceania\":7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "23a519c8",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome()\n",
    "scope = cast_net(region=\"Europe\",languages=\"English,Aramaic,French\",recency=\"2\")\n",
    "browser.get(scope)\n",
    "time.sleep(10)\n",
    "anchors = browser.find_elements(By.TAG_NAME, \"a\")\n",
    "urls = [anchor.get_attribute('href') for anchor in anchors]\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5dd5234d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix = \"https://www.developmentaid.org/jobs/view/\"\n",
    "filtered_list = [url for url in urls if url is not None and url.startswith(prefix)]\n",
    "unique_list = []\n",
    "[unique_list.append(x) for x in filtered_list if x not in unique_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "25226eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_opportunities = unique_list\n",
    "opportunities_dict = {}\n",
    "driver = webdriver.Chrome()\n",
    "for opportunity in relevant_opportunities:\n",
    "    driver.get(opportunity)\n",
    "    try:\n",
    "        accept_button = WebDriverWait(driver, 1).until(\n",
    "            EC.element_to_be_clickable((By.ID, 'acceptCookies')))  \n",
    "        accept_button.click()\n",
    "    except:\n",
    "        pass\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "    page_text = soup.get_text(separator=' ', strip=True)\n",
    "    opportunities_dict[opportunity] = page_text\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "66b1b232",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[109], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m opportunity \u001b[39min\u001b[39;00m opportunities_dict\u001b[39m.\u001b[39mvalues(): \n\u001b[0;32m----> 4\u001b[0m     info \u001b[39m=\u001b[39m get_info(opportunity)\n\u001b[1;32m      5\u001b[0m     op \u001b[39m=\u001b[39m {}\n\u001b[1;32m      6\u001b[0m     opportunity_dict[i] \u001b[39m=\u001b[39m op\n",
      "Cell \u001b[0;32mIn[97], line 9\u001b[0m, in \u001b[0;36mget_info\u001b[0;34m(description)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m q \u001b[39min\u001b[39;00m questions: \n\u001b[1;32m      7\u001b[0m     QA_input \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m'\u001b[39m: q,\n\u001b[1;32m      8\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mcontext\u001b[39m\u001b[39m'\u001b[39m: description}\n\u001b[0;32m----> 9\u001b[0m     outputs\u001b[39m.\u001b[39mappend(nlp(QA_input)[\u001b[39m'\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     10\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/pipelines/question_answering.py:393\u001b[0m, in \u001b[0;36mQuestionAnsweringPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m examples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_args_parser(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    392\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(examples, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(examples) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 393\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(examples[\u001b[39m0\u001b[39;49m], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    394\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(examples, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/pipelines/base.py:1132\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[1;32m   1131\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, ChunkPipeline):\n\u001b[0;32m-> 1132\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\n\u001b[1;32m   1133\u001b[0m         \u001b[39miter\u001b[39;49m(\n\u001b[1;32m   1134\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_iterator(\n\u001b[1;32m   1135\u001b[0m                 [inputs], num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1136\u001b[0m             )\n\u001b[1;32m   1137\u001b[0m         )\n\u001b[1;32m   1138\u001b[0m     )\n\u001b[1;32m   1139\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/pipelines/pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    124\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterator)\n\u001b[0;32m--> 125\u001b[0m processed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfer(item, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams)\n\u001b[1;32m    126\u001b[0m \u001b[39m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader_batch_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[39m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/pipelines/question_answering.py:543\u001b[0m, in \u001b[0;36mQuestionAnsweringPipeline.postprocess\u001b[0;34m(self, model_outputs, top_k, handle_impossible_answer, max_answer_len, align_to_words)\u001b[0m\n\u001b[1;32m    540\u001b[0m example \u001b[39m=\u001b[39m output[\u001b[39m\"\u001b[39m\u001b[39mexample\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    541\u001b[0m p_mask \u001b[39m=\u001b[39m output[\u001b[39m\"\u001b[39m\u001b[39mp_mask\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    542\u001b[0m attention_mask \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 543\u001b[0m     output[\u001b[39m\"\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mnumpy() \u001b[39mif\u001b[39;00m output\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    544\u001b[0m )\n\u001b[1;32m    546\u001b[0m starts, ends, scores, min_null_score \u001b[39m=\u001b[39m select_starts_ends(\n\u001b[1;32m    547\u001b[0m     start_, end_, p_mask, attention_mask, min_null_score, top_k, handle_impossible_answer, max_answer_len\n\u001b[1;32m    548\u001b[0m )\n\u001b[1;32m    550\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mis_fast:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "opportunity_dict = {}\n",
    "i = 0\n",
    "for opportunity in opportunities_dict.values(): \n",
    "    info = get_info(opportunity)\n",
    "    op = {}\n",
    "    opportunity_dict[i] = op\n",
    "    op['Language'] = info[0]\n",
    "    op['Education'] = info[1]\n",
    "    op['Seniority'] = info[2]\n",
    "    op['Location'] = info[3]\n",
    "    op['Duties'] = info[4]\n",
    "    op['Experience'] = info[5]\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d14d48",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m opportunity \u001b[39min\u001b[39;00m opportunities_dict\u001b[39m.\u001b[39mvalues(): \n\u001b[0;32m----> 5\u001b[0m     info \u001b[39m=\u001b[39m get_info(opportunity)\n\u001b[1;32m      6\u001b[0m     op \u001b[39m=\u001b[39m {}\n\u001b[1;32m      7\u001b[0m     opportunity_dict[i] \u001b[39m=\u001b[39m op\n",
      "Cell \u001b[0;32mIn[77], line 9\u001b[0m, in \u001b[0;36mget_info\u001b[0;34m(description)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m q \u001b[39min\u001b[39;00m questions: \n\u001b[1;32m      7\u001b[0m     QA_input \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m'\u001b[39m: q,\n\u001b[1;32m      8\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mcontext\u001b[39m\u001b[39m'\u001b[39m: description}\n\u001b[0;32m----> 9\u001b[0m     outputs\u001b[39m.\u001b[39mappend(nlp(QA_input)[\u001b[39m'\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     10\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/pipelines/question_answering.py:393\u001b[0m, in \u001b[0;36mQuestionAnsweringPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m examples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_args_parser(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    392\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(examples, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(examples) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 393\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(examples[\u001b[39m0\u001b[39;49m], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    394\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(examples, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/pipelines/base.py:1132\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[1;32m   1131\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, ChunkPipeline):\n\u001b[0;32m-> 1132\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\n\u001b[1;32m   1133\u001b[0m         \u001b[39miter\u001b[39;49m(\n\u001b[1;32m   1134\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_iterator(\n\u001b[1;32m   1135\u001b[0m                 [inputs], num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1136\u001b[0m             )\n\u001b[1;32m   1137\u001b[0m         )\n\u001b[1;32m   1138\u001b[0m     )\n\u001b[1;32m   1139\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/pipelines/pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    124\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterator)\n\u001b[0;32m--> 125\u001b[0m processed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfer(item, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams)\n\u001b[1;32m    126\u001b[0m \u001b[39m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader_batch_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[39m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/pipelines/question_answering.py:543\u001b[0m, in \u001b[0;36mQuestionAnsweringPipeline.postprocess\u001b[0;34m(self, model_outputs, top_k, handle_impossible_answer, max_answer_len, align_to_words)\u001b[0m\n\u001b[1;32m    540\u001b[0m example \u001b[39m=\u001b[39m output[\u001b[39m\"\u001b[39m\u001b[39mexample\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    541\u001b[0m p_mask \u001b[39m=\u001b[39m output[\u001b[39m\"\u001b[39m\u001b[39mp_mask\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    542\u001b[0m attention_mask \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 543\u001b[0m     output[\u001b[39m\"\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mnumpy() \u001b[39mif\u001b[39;00m output\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    544\u001b[0m )\n\u001b[1;32m    546\u001b[0m starts, ends, scores, min_null_score \u001b[39m=\u001b[39m select_starts_ends(\n\u001b[1;32m    547\u001b[0m     start_, end_, p_mask, attention_mask, min_null_score, top_k, handle_impossible_answer, max_answer_len\n\u001b[1;32m    548\u001b[0m )\n\u001b[1;32m    550\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mis_fast:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(opportunity_dict, orient='index')\n",
    "excel_filename = 'Opportunities Summary'\n",
    "current_time = datetime.now()\n",
    "formatted_timestamp = current_time.strftime(\"%Y-%m-%d\")\n",
    "new_string = excel_filename + \" \" + formatted_timestamp+'.xlsx'\n",
    "df.to_excel(new_string, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f58a125",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
